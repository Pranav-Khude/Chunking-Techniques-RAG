{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text=\"Why did ther scarecrow win an award? \" \\\n",
    "\"Because he was outstanding in his field! This is the example trext for this fun exercise.\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35,chunk_overlap=2,separator='')\n",
    "documents=text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did ther scarecrow win an award\n",
      "rd? Because he was outstanding in h\n",
      "his field! This is the example tre\n",
      "rext for this fun exercise.\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Recursive Character Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text=\"Why did the scarecrow win an award? \" \\\n",
    "\"Because he was outstanding in his field! This is the example text for this fun exercise.\" \\\n",
    "\"Another sentence for the example text.\" \\\n",
    "\"Splitting text can be fun and educational.\"\n",
    "\n",
    "splitter1 = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=0)\n",
    "documents1=splitter1.create_documents([text])\n",
    "\n",
    "splitter2 = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=0)\n",
    "documents2=splitter2.create_documents([text])\n",
    "\n",
    "splitter3 = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=10)\n",
    "documents3=splitter3.create_documents([text])\n",
    "\n",
    "splitter4 = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "documents4=splitter4.create_documents([text])\n",
    "\n",
    "\n",
    "def print_documents(documents,config_number):\n",
    "    print(f\"config {config_number}\")\n",
    "    for i, doc in enumerate(documents,1):\n",
    "        print(f\"Chunk{i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 1\n",
      "Chunk1: Why did the scarecrow win an award? Because he was\n",
      "Chunk2: outstanding in his field! This is the example\n",
      "Chunk3: text for this fun exercise.Another sentence for\n",
      "Chunk4: the example text.Splitting text can be fun and\n",
      "Chunk5: educational.\n",
      "config 2\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field! This is the example\n",
      "Chunk2: text for this fun exercise.Another sentence for the example text.Splitting text can be fun and\n",
      "Chunk3: educational.\n",
      "config 3\n",
      "Chunk1: Why did the scarecrow win an award? Because he was\n",
      "Chunk2: he was outstanding in his field! This is the\n",
      "Chunk3: is the example text for this fun exercise.Another\n",
      "Chunk4: sentence for the example text.Splitting text can\n",
      "Chunk5: text can be fun and educational.\n",
      "config 4\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field! This is the example\n",
      "Chunk2: This is the example text for this fun exercise.Another sentence for the example text.Splitting text\n",
      "Chunk3: text.Splitting text can be fun and educational.\n"
     ]
    }
   ],
   "source": [
    "print_documents(documents1,1)\n",
    "print_documents(documents2,2)\n",
    "print_documents(documents3,3)\n",
    "print_documents(documents4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 1\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field!\n",
      "Chunk2: This is the example text for this fun exercise. Another sentence to add more variety.\n",
      "Chunk3: Splitting text can be fun and educational.\n",
      "config 2\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field! This is the example text for this fun exercise.\n",
      "Chunk2: This is the example text for this fun exercise. Another sentence to add more variety. Splitting text can be fun and educational.\n",
      "Chunk3: Splitting text can be fun and educational.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "class SpacySentenceTokenizer:\n",
    "    def __init__(self,stride=2,overlap=0):\n",
    "        self.stride=stride\n",
    "        self.overlap=overlap\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def create_documents(self,text):\n",
    "        doc=self.nlp(text)\n",
    "        sentences=[sent.text for sent in doc.sents]\n",
    "        chunks=[]\n",
    "\n",
    "        start=0\n",
    "        while start<len(sentences):\n",
    "            end=start+self.stride\n",
    "            chunk=sentences[start:end]\n",
    "            chunks.append(' '.join(chunk))\n",
    "            start+=self.stride-self.overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "text=\"Why did the scarecrow win an award? \" \\\n",
    "\"Because he was outstanding in his field! This is the example text for this fun exercise.\" \\\n",
    "\"Another sentence to add more variety.\" \\\n",
    "\"Splitting text can be fun and educational.\"\n",
    "\n",
    "# configuration I: Stride of 2 sentences, overlap of  0 sentences\n",
    "tokenizer1=SpacySentenceTokenizer(stride=2,overlap=0)\n",
    "documents1=tokenizer1.create_documents(text)\n",
    "\n",
    "# configuration II: Stride of 3 sentences, overlap of 1 sentence\n",
    "tokenizer2=SpacySentenceTokenizer(stride=3,overlap=1)\n",
    "documents2=tokenizer2.create_documents(text)\n",
    "\n",
    "def print_documents(documents,config_number):\n",
    "    print(f\"config {config_number}\")\n",
    "    for i, doc in enumerate(documents,1):\n",
    "        print(f\"Chunk{i}: {doc}\")\n",
    "\n",
    "print_documents(documents1,1)\n",
    "print_documents(documents2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 1\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field! This is the example text for this fun exercise.\n",
      "Chunk2: Another sentence to add more variety.Splitting text can be fun and educational.\n",
      "Chunk3: Here's another sentence.\n",
      "Chunk4: And one more to check the clustering..\n",
      "config 2\n",
      "Chunk1: Why did the scarecrow win an award? Because he was outstanding in his field! This is the example text for this fun exercise. Another sentence to add more variety.Splitting text can be fun and educational.\n",
      "Chunk2: Here's another sentence.\n",
      "Chunk3: And one more to check the clustering..\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "class SimilarSentenceSplitter:\n",
    "    def __init__(self,group_max_sentences=3,similarity_threshold=0.8):\n",
    "        self.group_max_sentences=group_max_sentences\n",
    "        self.similarity_threshold=similarity_threshold\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def create_documents(self,text):\n",
    "        sentences=[sent.strip() + '.' for sent in text.split('. ') if sent]\n",
    "        embeddings=self.model.encode(sentences,convert_to_tensor=True)\n",
    "        chunks,current_chunk=[],[sentences[0]]\n",
    "\n",
    "        for i in range(1,len(sentences)):\n",
    "            if len(current_chunk)>=self.group_max_sentences or util.pytorch_cos_sim(embeddings[i-1],embeddings[i]).item()<self.similarity_threshold:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk=[sentences[i]]\n",
    "            else:\n",
    "                current_chunk.append(sentences[i])\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    \n",
    "text=(\"Why did the scarecrow win an award? Because he was outstanding in his field! \"\n",
    "\"This is the example text for this fun exercise. Another sentence to add more variety.\"\n",
    "\"Splitting text can be fun and educational. Here's another sentence. And one more to check the clustering.\")\n",
    "\n",
    "splitter1=SimilarSentenceSplitter(group_max_sentences=3,similarity_threshold=0.8)\n",
    "documents1=splitter1.create_documents(text)\n",
    "\n",
    "splitter2=SimilarSentenceSplitter(group_max_sentences=2,similarity_threshold=0.2)\n",
    "documents2=splitter2.create_documents(text)\n",
    "\n",
    "\n",
    "def print_documents(documents,config_number):\n",
    "    print(f\"config {config_number}\")\n",
    "    for i, doc in enumerate(documents,1):\n",
    "        print(f\"Chunk{i}: {doc}\")\n",
    "\n",
    "print_documents(documents1,1)\n",
    "print_documents(documents2,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
